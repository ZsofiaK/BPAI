{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different ML models\n",
    "Random forest regressor, gradient boosting regressor, zero-inflated beta regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklego.meta import ZeroInflatedRegressor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.othermod.betareg import BetaModel\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxilliary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(data_role:str, data_X:np.ndarray, data_y:np.ndarray, noSupply_value:float, points:int) -> np.ndarray:\n",
    "    '''Creates a dataset from raw data according to a 5-points-wide aggregated sliding window. Assumes no header rows.\n",
    "    :param: data_role: whether the array to aggregate is the predictor (\"X\") or target (\"y\")\n",
    "    :param: data_X: the predictor columns of the dataset to aggregate\n",
    "    :param: data_Y: the target columns of the dataset to aggregate\n",
    "    :param: noSupply_value: the value to substitute in the target variable if there was no shoot supply\n",
    "    :param: points: the number of sampling points within a forest track.\n",
    "    :return: the aggregated data array.\n",
    "    '''\n",
    "\n",
    "    if data_role == 'X':\n",
    "        processed_data = data_X.copy()\n",
    "\n",
    "        processed_data = processed_data.astype(float)\n",
    "\n",
    "        rows, cols = np.shape(data_X)\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(0, cols, 3):\n",
    "                # Choosing 5 adjacent points within the limit of one forest.\n",
    "                start_row = max(i - 2, int((i // points * points)))\n",
    "                end_row = min(i + 2, int((i // points + 1) * points - 1))\n",
    "\n",
    "                processed_data[i, j] = np.sum(data_X[start_row:end_row+1, j].astype(float))\n",
    "\n",
    "    elif data_role == 'y':\n",
    "        processed_data = data_y.copy()\n",
    "\n",
    "        processed_data = processed_data.astype(float)\n",
    "\n",
    "        rows, cols = np.shape(data_y)\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                # Choosing 5 adjacent points within the limit of one forest.\n",
    "                start_row = max(i - 2, int((i // points * points)))  \n",
    "                end_row = min(i + 2, int((i // points + 1) * points - 1))\n",
    "\n",
    "                aggregated_supply = np.sum(data_X[start_row:end_row+1, j * 3].astype(float))\n",
    "\n",
    "                if aggregated_supply > 0:\n",
    "                    aggregated_browsed = sum([(float(data_X[row,j*3]) / float(aggregated_supply)) * float(data_y[row,j]) \\\n",
    "                        for row in range(start_row, end_row+1)])\n",
    "\n",
    "                    processed_data[i,j] = aggregated_browsed\n",
    "\n",
    "                else:\n",
    "                    processed_data[i, j] = noSupply_value\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def normalize_TS(X:np.ndarray) -> np.ndarray:\n",
    "    '''Normalizes the TS columns of a predictor array.\n",
    "    :param: X: the array which contains the columns to normalize.\n",
    "    :return: the complete array in which the TS columns are now normalized.'''\n",
    "\n",
    "    X_normalized = X.copy()\n",
    "\n",
    "    cols_to_normalize = [i for i in range(np.shape(X)[1]) if i % 3 == 0]\n",
    "\n",
    "    smoothing = 0.00001     # Applied so that 0 division does not cause any problems while normalizing.\n",
    "\n",
    "    X_normalized[:, cols_to_normalize] = (X[:, cols_to_normalize] - (X[:, cols_to_normalize].min(axis=0))) \\\n",
    "                            / (X[:, cols_to_normalize].max(axis=0) - (X[:, cols_to_normalize].min(axis=0)) + smoothing)\n",
    "    \n",
    "    return X_normalized\n",
    "\n",
    "def adjust_predictions(y_pred:np.ndarray, X:np.ndarray, noSupply_value:float) -> np.ndarray:\n",
    "    '''Ensures that the specified noSupply_value is predicted whenever there is no shoot supply\n",
    "    and that there are no negative predictions when there IS supply.\n",
    "    :param: y_pred: the array of original predictions.\n",
    "    :param: X: the array of predictors.\n",
    "    :param: noSupply_value: the value to predict in case of no supply.\n",
    "    :return: the adjusted array of predictions.\n",
    "    '''\n",
    "\n",
    "    y_pred_adjusted = y_pred.copy()\n",
    "\n",
    "    for i in range(y_pred.shape[0]):\n",
    "            for j in range(y_pred.shape[1]):\n",
    "                if X[i, j * 3] == 0:\n",
    "                    y_pred_adjusted[i, j] = noSupply_value\n",
    "\n",
    "                elif y_pred[i, j] < 0:\n",
    "                    y_pred_adjusted[i, j] = 0\n",
    "\n",
    "    return y_pred_adjusted\n",
    "\n",
    "def csv_to_dict(filename:str) -> Dict[any,any]:\n",
    "    '''Reads a csv file to a dictionary. Keys come from the first column, values from the second.\n",
    "    :param: filename: name of the file to read, must include path if in different directory.\n",
    "    :return: a dictionary constructed from the csv file.'''\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            if len(row) >= 2:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "\n",
    "                if value == 'None':\n",
    "                    value = None\n",
    "\n",
    "                elif '.' in value:\n",
    "                    value = float(value)\n",
    "\n",
    "                else:\n",
    "                    value = int(value)\n",
    "\n",
    "                result_dict[key] = value\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def reduce_data(X_array:np.ndarray, y_array, proportion:float, seed:int) -> np.ndarray:\n",
    "    '''Reduces forest browsing data by a specified proportion using a random seed.\n",
    "    :param: X_array: numpy array containing the predictors of the data.\n",
    "    :param: y_array: numpy array containing freshly browsed data.\n",
    "    :param: proportion: the proportion to reduce the data by.\n",
    "    :param: seed: the seed for pseudo-random row selection.\n",
    "    :return: the reduced data array.\n",
    "    '''\n",
    "\n",
    "    if proportion == 0.0:\n",
    "        return X_array, y_array\n",
    "    \n",
    "    rows_per_track = 100\n",
    "    rows_to_remove = int(rows_per_track * proportion)\n",
    "    nr_tracks = len(X_array) // rows_per_track\n",
    "\n",
    "    keep_indices = np.ones(len(X_array), dtype=bool)\n",
    "\n",
    "    for i in range(nr_tracks):\n",
    "        start_index = i * rows_per_track\n",
    "        end_index = start_index + rows_per_track\n",
    "\n",
    "        # Generate random indices within the current track\n",
    "        random.seed(seed)\n",
    "        indices_to_remove = random.sample(range(start_index, end_index), rows_to_remove)\n",
    "\n",
    "        # Set the indices to remove to False in keep_indices\n",
    "        keep_indices[indices_to_remove] = False\n",
    "\n",
    "    # Seeting random seed back to system time.\n",
    "    random.seed(None)\n",
    "\n",
    "    # Filter the array using the keep_indices\n",
    "    filtered_X, filtered_y = X_array[keep_indices, :], y_array[keep_indices]\n",
    "\n",
    "    return filtered_X, filtered_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_statsmodels = sm.add_constant(X)\n",
    "        \n",
    "        self.model = BetaModel(y, X_statsmodels)\n",
    "\n",
    "        self.results = self.model.fit()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_statsmodels = sm.add_constant(X)\n",
    "        \n",
    "        return self.results.predict(X_statsmodels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the previously selected hyperparameters for the models.\n",
    "rf_hp = csv_to_dict('./CV/rf.csv')\n",
    "gb_hp = csv_to_dict('./CV/gb.csv')\n",
    "br_hp = csv_to_dict('./CV/br.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Random Forest Regressor', 'Gradient Boosting Regressor', 'Zero-Inflated Beta Regressor']\n",
    "models = [RandomForestRegressor(n_estimators=rf_hp['best_n_estimators'], max_depth=rf_hp['best_max_depth'], max_samples=rf_hp['best_max_samples'], random_state=23), \\\n",
    "          GradientBoostingRegressor(learning_rate=gb_hp['best_rate'], n_estimators=gb_hp['best_n_estimators'], max_depth=gb_hp['best_max_depth'], random_state=23), \\\n",
    "          ZeroInflatedRegressor(classifier=SVC(C=br_hp['best_C']), regressor=BetaRegressor())\n",
    "        ]\n",
    "sample_sizes = [1.0, 0.75, 0.5]\n",
    "seeds = [i for i in range(50)]\n",
    "\n",
    "noSupply_value = -0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv('./Data/predictors_fl.csv')\n",
    "y_df = pd.read_csv('./Data/freshly_browsed_dist_fl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_X = X_df.values\n",
    "og_y = y_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for label in labels:\n",
    "    results[label] = {}\n",
    "    for size in sample_sizes:\n",
    "        results[label][size] = {'y_pred' : [], 'errors' : []}\n",
    "\n",
    "data = {}\n",
    "\n",
    "for size in sample_sizes:\n",
    "    data[size] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0%\n"
     ]
    }
   ],
   "source": [
    "total_datas = len(sample_sizes) * len(seeds)\n",
    "progress_counter = 0\n",
    "\n",
    "for size in sample_sizes:\n",
    "    for seed in seeds:\n",
    "        X_unagg, y_unagg = reduce_data(og_X, og_y, (1-size), seed)\n",
    "\n",
    "        X = aggregate('X', X_unagg, y_unagg, noSupply_value, 100 * size)\n",
    "        y = aggregate('y', X_unagg, y_unagg, noSupply_value, 100 * size)\n",
    "\n",
    "        X = normalize_TS(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=200, random_state=23)\n",
    "\n",
    "        data[size].append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "        progress_counter += 1\n",
    "        progress_percent = progress_counter / total_datas * 100\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f'Progress: {progress_percent:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_labels = ['Xtrain', 'Xtest', 'ytrain', 'ytest']\n",
    "\n",
    "for size, variations in data.items():\n",
    "    size_label = ''.join(str(size).split('.'))\n",
    "    \n",
    "    for index, variation in enumerate(variations):\n",
    "\n",
    "        for j, label in enumerate(array_labels):\n",
    "            np.savetxt(f'./Data for testing/{size_label}_{index}_{label}.csv', variation[j], delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress for 3. model, Zero-Inflated Beta Regressor: 100.0%\n"
     ]
    }
   ],
   "source": [
    "total_runs = len(sample_sizes) * len(seeds)\n",
    "\n",
    "for index, label in enumerate(labels):\n",
    "        progress_counter = 0\n",
    "\n",
    "        for size in sample_sizes:\n",
    "                for seed_index, seed in enumerate(seeds):\n",
    "                        X_train, X_test, y_train, y_test = data[size][seed_index]        \n",
    "\n",
    "                        model = models[index]\n",
    "\n",
    "                        if label == 'Random Forest Regressor':\n",
    "\n",
    "                                model.fit(X_train, y_train)\n",
    "\n",
    "                                y_pred_test = model.predict(X_test)\n",
    "\n",
    "                                y_pred_test = adjust_predictions(y_pred_test, X_test, noSupply_value)\n",
    "\n",
    "                        elif label == 'Gradient Boosting Regressor':\n",
    "                                y_pred_test = np.zeros_like(y_test)\n",
    "\n",
    "                                for i in range(np.shape(y_test)[1]):\n",
    "                                        model.fit(X_train, y_train[:,i])\n",
    "                                        y_pred_test[:,i] = model.predict(X_test)\n",
    "\n",
    "                                y_pred_test = adjust_predictions(y_pred_test, X_test, noSupply_value)\n",
    "                        \n",
    "                        elif label == 'Zero-Inflated Beta Regressor':\n",
    "                                # Exchanging ones to a value very close to 1 since beta regression cannot handle exactly 1.0 values.\n",
    "                                y_train_beta = y_train.copy()\n",
    "                                y_train_beta[y_train_beta == 1.0] = 1.0 - (-noSupply_value)\n",
    "                                \n",
    "                                y_test_beta = y_test.copy()\n",
    "                                y_test_beta[y_test_beta == 1.0] = 1.0 - (-noSupply_value)\n",
    "\n",
    "                                y_pred_test = np.zeros_like(y_test)\n",
    "\n",
    "                                total_models = np.shape(y_pred_test)[1]\n",
    "\n",
    "                                SVC_zeros = 0\n",
    "\n",
    "                                for i in range(np.shape(y_test_beta)[1]):\n",
    "                                        # Using only rows where there WAS supply from the species.\n",
    "                                        reg_rows = [r for r in range(np.shape(y_test_beta)[0]) if y_test_beta[r, i] >= 0]\n",
    "                                        noReg_rows = [r for r in range(np.shape(y_test_beta)[0]) if r not in reg_rows]\n",
    "\n",
    "                                        X_train_reg = X_train[reg_rows]\n",
    "                                        y_train_reg = y_train_beta[reg_rows, i]\n",
    "                                        \n",
    "                                        X_test_reg = X_test[reg_rows]\n",
    "\n",
    "                                        try:    # Runs into error if all target values are 0.\n",
    "                                                model.fit(X_train_reg, y_train_reg)\n",
    "                                                y_pred_test[reg_rows,i] = model.predict(X_test_reg)\n",
    "\n",
    "                                        except:\n",
    "                                                pass    # Nothing to do as default prediction is 0.\n",
    "\n",
    "                                        # Setting prediction for rows with no supply.\n",
    "                                        y_pred_test[noReg_rows, i] = noSupply_value\n",
    "\n",
    "                        \n",
    "                        errors = y_pred_test - y_test\n",
    "\n",
    "                        results[label][size]['y_pred'].append(y_pred_test)\n",
    "                        results[label][size]['errors'].append(errors)\n",
    "\n",
    "                        progress_counter += 1\n",
    "                        progress_percent = progress_counter / total_datas * 100\n",
    "\n",
    "                        clear_output(wait=True)\n",
    "                        print(f'Progress for {index+1}. model, {label}: {progress_percent:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, size_dict in results.items():\n",
    "    if label == 'Random Forest Regressor':\n",
    "        model_name = 'rf'\n",
    "\n",
    "    elif label == 'Gradient Boosting Regressor':\n",
    "        model_name = 'gb'\n",
    "\n",
    "    elif label == 'Zero-Inflated Beta Regressor':\n",
    "        model_name = 'br'\n",
    "\n",
    "    for size, result in size_dict.items():\n",
    "        size_name = ''.join(str(size).split('.'))\n",
    "        \n",
    "        for component, arrays in result.items():\n",
    "            for i, array in enumerate(arrays):\n",
    "                np.savetxt(f'./Results/{model_name}_{size_name}_{i}_{component}.csv', array, delimiter=',', fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
