{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "from typing import List, Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxilliary functions\n",
    "The following section defines some functions which will be later used in processing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_forest_data(file_name:str) -> np.ndarray:\n",
    "    '''Reads the data on plant browsing from an Excel sheet into a numpy array.\n",
    "    \n",
    "    :param: filename: a string containing the name of the Excel file.\n",
    "    :return: a numpy array containing the data.\n",
    "    '''\n",
    "\n",
    "    # Constants used in the function:\n",
    "    START_COLUMN = 'P'                          # The column in which the data starts in the Excel sheet.\n",
    "    NAME_INDEX = 1                              # Index of the column with the first species name.\n",
    "    SAMPLING_POINTS = 100                       # Number of sampling points.\n",
    "    SHEET_NAME = 'terepi-hajtásszám&hullaték'\n",
    "\n",
    "    df = pd.read_excel(file_name, sheet_name=SHEET_NAME, skiprows=3, usecols=f'{START_COLUMN}:ZZZ', header=None, engine='openpyxl').reset_index(drop=True)\n",
    "\n",
    "    last_column = df.iloc[1].tolist().index('Kínált hajtások száma az egyes pontokban') - 1\n",
    "\n",
    "    # Initialize the numpy array\n",
    "    num_rows = SAMPLING_POINTS + 3              # 100 sampling points + 3 headers\n",
    "    num_cols = (last_column + 1)\n",
    "    data = np.zeros((num_rows, num_cols), dtype=object)\n",
    "\n",
    "    # Populate the array with values from the dataframe.\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # Species names are inserted into the first row.\n",
    "        if i == 0:\n",
    "            name_index = NAME_INDEX\n",
    "\n",
    "            while name_index < last_column:\n",
    "                data[i, name_index-1:name_index+8] = df.iloc[0,name_index]\n",
    "\n",
    "                name_index += 9\n",
    "\n",
    "        # Count headers are inserted into the second row.\n",
    "        elif i == 1:\n",
    "            repeats = num_cols / 9\n",
    "\n",
    "            iteration = 0\n",
    "\n",
    "            while iteration < repeats:\n",
    "                start_point = iteration * 9\n",
    "\n",
    "                data[i, start_point:start_point+4] = 'Összes hajtásvég'\n",
    "                data[i, start_point+4:start_point+8] = 'Friss rágott hajtásvég'\n",
    "                data[i, start_point+8] = 'Régi rágott hajtásvég'\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "        \n",
    "        # The rest of the dataframe is copied into the array.\n",
    "        else:\n",
    "            data[i, :] = df.iloc[i, :last_column + 1].values\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/fn5x2gps4wq728lxf9qrv_lm0000gn/T/ipykernel_40721/2886446634.py:14: FutureWarning: Defining usecols with out of bounds indices is deprecated and will raise a ParserError in a future version.\n",
      "  df = pd.read_excel(file_name, sheet_name=SHEET_NAME, skiprows=3, usecols=f'{START_COLUMN}:ZZZ', header=None, engine='openpyxl').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Testing read_forest_data.\n",
    "test_data = read_forest_data('Forests/Nyugat_Mátra/Ny_Mátra 2 vonal.xlsx')\n",
    "np.savetxt('./Tests/test_read_forest_data.csv', test_data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "\n",
    "def aggregate_data(data:np.ndarray) -> np.ndarray:\n",
    "    '''Aggregates height levels in forest browsing data.\n",
    "    \n",
    "    :param: data: numpy array containing unaggregated data.\n",
    "    :return: a numpy array containing the aggregated data.\n",
    "    '''\n",
    "\n",
    "    # Initialize new array.\n",
    "    nr_rows = np.shape(data)[0] - 2             # Two less rows as original data since no height level and measurement headers are removed.\n",
    "    \n",
    "    nr_species = int(np.shape(data)[1] / 9)\n",
    "    nr_columns = nr_species * 3\n",
    "\n",
    "    aggregated = np.zeros((nr_rows, nr_columns), dtype=object)\n",
    "\n",
    "    # Get species names.\n",
    "    unique_species, indices = np.unique(data[0], return_index=True)\n",
    "    species = unique_species[np.argsort(indices)].tolist()\n",
    "\n",
    "    for i in range(nr_rows):\n",
    "        if i == 0:\n",
    "            for index, sp in enumerate(species):\n",
    "                start = index * 3\n",
    "                aggregated[i, start] = sp + ' all shoots'\n",
    "                aggregated[i, start+1] = sp + ' freshly browsed'\n",
    "                aggregated[i, start+2] = sp + ' old browsed'\n",
    "\n",
    "        else:\n",
    "            for j in range(nr_species):\n",
    "                start_unaggregated = j * 9\n",
    "                start_aggregated = j * 3\n",
    "\n",
    "                aggregated_shoots = np.sum(data[i + 2, start_unaggregated:start_unaggregated+4])\n",
    "                aggregated_browsed = np.sum(data[i + 2, start_unaggregated+4:start_unaggregated+8])\n",
    "\n",
    "                if aggregated_browsed > aggregated_shoots:\n",
    "                    log.append(f'{species[j]}\\t ROW {i}\\t ALL: {aggregated_shoots}\\t FRESH: {aggregated_browsed}')\n",
    "                    \n",
    "                    # If more browsed than offered is recorded, the number offered is corrected to offered + browsed.\n",
    "                    aggregated[i, start_aggregated] = aggregated_shoots + aggregated_browsed\n",
    "\n",
    "                else:\n",
    "                    aggregated[i, start_aggregated] = aggregated_shoots\n",
    "\n",
    "                aggregated[i, start_aggregated + 1] = aggregated_browsed\n",
    "                \n",
    "                aggregated[i, start_aggregated + 2] = data[i + 2, start_unaggregated+8]\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing aggregate_data.\n",
    "test_aggregated = aggregate_data(test_data)\n",
    "np.savetxt('./Tests/test_aggregate_data.csv', test_aggregated, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_forest(paths:List[np.ndarray]) -> np.ndarray:\n",
    "    '''Combines data from different sampling paths within a forest into a single numpy array.\n",
    "    This function assumes that the sampling paths have the same species recorded in the same order.\n",
    "    \n",
    "    :param: paths: a list containing numpy arrays with the data from the different paths.\n",
    "    :return: a numpy array with the combined forest data.'''\n",
    "\n",
    "    combined = paths[0]\n",
    "\n",
    "    if len(paths) > 1:\n",
    "        for array in paths[1:]:\n",
    "            combined = np.concatenate((combined, array[1:,:]))\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/fn5x2gps4wq728lxf9qrv_lm0000gn/T/ipykernel_40721/2886446634.py:14: FutureWarning: Defining usecols with out of bounds indices is deprecated and will raise a ParserError in a future version.\n",
      "  df = pd.read_excel(file_name, sheet_name=SHEET_NAME, skiprows=3, usecols=f'{START_COLUMN}:ZZZ', header=None, engine='openpyxl').reset_index(drop=True)\n",
      "/var/folders/t3/fn5x2gps4wq728lxf9qrv_lm0000gn/T/ipykernel_40721/2886446634.py:14: FutureWarning: Defining usecols with out of bounds indices is deprecated and will raise a ParserError in a future version.\n",
      "  df = pd.read_excel(file_name, sheet_name=SHEET_NAME, skiprows=3, usecols=f'{START_COLUMN}:ZZZ', header=None, engine='openpyxl').reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the separate arrays:\n",
      "(101, 69)\n",
      "(101, 69)\n",
      "(101, 69)\n",
      "Shape of the combined array: (301, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/fn5x2gps4wq728lxf9qrv_lm0000gn/T/ipykernel_40721/2886446634.py:14: FutureWarning: Defining usecols with out of bounds indices is deprecated and will raise a ParserError in a future version.\n",
      "  df = pd.read_excel(file_name, sheet_name=SHEET_NAME, skiprows=3, usecols=f'{START_COLUMN}:ZZZ', header=None, engine='openpyxl').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Testing combine_forest.\n",
    "import math\n",
    "\n",
    "test_paths = []\n",
    "test_directory = 'Forests/Nyugat_Mátra/'\n",
    "\n",
    "for file_name in os.listdir(test_directory):\n",
    "    if file_name.endswith('xlsx'):\n",
    "            read_data = read_forest_data(test_directory + file_name)\n",
    "\n",
    "            test_paths.append(aggregate_data(read_data))\n",
    "\n",
    "test_combined = combine_forest(test_paths)\n",
    "\n",
    "np.savetxt('./Tests/test_combine_forest.csv', test_combined, fmt='%s', delimiter=',')\n",
    "\n",
    "print('Shapes of the separate arrays:')\n",
    "for path in test_paths:\n",
    "      print(np.shape(path))\n",
    "print('Shape of the combined array:', np.shape(test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(data: np.ndarray, file_name: str, property_count: int) -> None:\n",
    "    '''Checks data for recording errors.\n",
    "    :param: data: numpy array containing recorded data.\n",
    "    :param: file_name: string containing the file name from which the data was read.\n",
    "    :param: property_count: integer showing the number of properties read by species.\n",
    "    '''\n",
    "\n",
    "    nr_species = int(np.shape(data)[1] / property_count)\n",
    "\n",
    "    # Checking for smaller all shoot count than freshly browsed shoots.\n",
    "    # Assumes that all shoots is the first property per species and freshly browsed is the second.\n",
    "    for i in range(1, len(data)):\n",
    "        for j in range(nr_species):\n",
    "            if data[i, j*property_count] < data[i, j*property_count + 1]:\n",
    "                print(file_name, data[0, j*property_count], i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "The following cell reads the data files into a dictionary in which the keys are names of the forests, and the values are numpy arrays containing the height-level aggregated browsing data from each of the 3 sampling paths in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "forests: Dict[str, np.ndarray] = {}\n",
    "\n",
    "root_folder = './Forests/'\n",
    "\n",
    "log = []\n",
    "\n",
    "for folder_name in os.listdir(root_folder):\n",
    "    folder_path = os.path.join(root_folder, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        paths_data = []\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('xlsx'):\n",
    "                    try:\n",
    "                        log.append('\\n' + file_name)\n",
    "                        \n",
    "                        path_data = aggregate_data(read_forest_data(folder_path + '/' + file_name))\n",
    "\n",
    "                        paths_data.append(path_data)\n",
    "\n",
    "                    except:\n",
    "                        warnings.warn(f'Problem with loading {file_name}.', UserWarning)\n",
    "\n",
    "        combined = combine_forest(paths_data)\n",
    "\n",
    "        forests[folder_name] = combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('log.txt', 'w') as file:\n",
    "    for item in log:\n",
    "        file.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest regions:\n",
      "['Gyöngyöstarján Világos-hegy', 'Mátrabérc-Fallóskút', 'Recsk', 'Gyöngyöspata Havas', 'Gyöngyös Sár-hegy', 'Nyugat_Mátra', 'Mátra-Észak']\n",
      "\n",
      "Total sampling points:\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "# Saving dictionary duplicate so that the reading does not need to be done again.\n",
    "forests_saved = forests.copy()\n",
    "\n",
    "# Printing summary of read data:\n",
    "\n",
    "print('Forest regions:')\n",
    "print(list(forests.keys()), end='\\n\\n')\n",
    "\n",
    "print('Total sampling points:')\n",
    "print(sum([np.shape(data)[0] - 1 for data in forests.values()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating species distribution properties within forests\n",
    "Species coverage and % of all shoots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each forest, a table is created, where for each species\n",
    "# the species coverage (% sampling points) and the species\n",
    "# % contribution to all shoots in the forest is stored.\n",
    "\n",
    "import math\n",
    "\n",
    "distributions: Dict[str, np.ndarray] = {}\n",
    "\n",
    "for forest, data in forests.items():\n",
    "    sampling_points = len(data) - 1\n",
    "    all_shoots = 0\n",
    "\n",
    "    unique_species, indices = np.unique([' '.join(cell.split(' ')[:-2]) for cell in data[0, :]], return_index=True)\n",
    "    species = unique_species[np.argsort(indices)]\n",
    "    nr_species = len(species)\n",
    "\n",
    "    distributions[forest] = np.concatenate((species[np.newaxis, :].astype(object), np.zeros((2, nr_species))))\n",
    "\n",
    "    for i in range(1,len(data)):\n",
    "        for j in range(nr_species):\n",
    "            offered_shoots = data[i, j*3]\n",
    "            \n",
    "            if offered_shoots != 0:\n",
    "                distributions[forest][1,j] += 1\n",
    "                distributions[forest][2,j] += offered_shoots\n",
    "\n",
    "                all_shoots += offered_shoots\n",
    "    \n",
    "    #Normalizing species data with nr of sampling points / shoots in the forest.\n",
    "    coverages = distributions[forest][1, :] / sampling_points\n",
    "    supplyPerces = distributions[forest][2, :] / all_shoots\n",
    "\n",
    "    distributions[forest][1, :] = coverages\n",
    "    distributions[forest][2, :] = supplyPerces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding distribution properties to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coverage and % of all shoots for each species is inserted at each sampling point in each forest.\n",
    "\n",
    "for forest, data in forests.items():\n",
    "\n",
    "    coverage_values = distributions[forest][1, :][np.newaxis, :]\n",
    "    supplyPerc_values = distributions[forest][2, :][np.newaxis, :]\n",
    "    \n",
    "    species = distributions[forest][0, :][np.newaxis, :]\n",
    "    nr_species = np.shape(species)[1]\n",
    "\n",
    "    updated_data = data\n",
    "\n",
    "    for i in range(nr_species):\n",
    "        index_coverage = i * 3 + 3 + i*2\n",
    "        index_supplyPerc = i * 3 + 4 + i*2\n",
    "\n",
    "        #species_coverage = np.array([species[0, i] + ' forest coverage'])[np.newaxis, 1]\n",
    "        #species_supplyPerc = np.array([species[0, i] + ' supply percentage'])[np.newaxis, 1]\n",
    "\n",
    "        coverage_value = coverage_values[0, i]\n",
    "        coverage_array = np.tile(coverage_value, (np.shape(updated_data)[0], 1)).astype(object)\n",
    "        coverage_array[0,0] = species[0, i] + ' forest coverage'\n",
    "\n",
    "        supplyPerc_value = supplyPerc_values[0, i]\n",
    "        supplyPerc_array = np.tile(supplyPerc_value, (np.shape(updated_data)[0], 1)).astype(object)\n",
    "        supplyPerc_array[0,0] = species[0, i] + ' supply percentage'\n",
    "\n",
    "        updated_data = np.insert(updated_data, [index_coverage], coverage_array, axis=1)\n",
    "        updated_data = np.insert(updated_data, [index_supplyPerc], supplyPerc_array, axis=1)\n",
    "\n",
    "    forests[forest] = updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distribution properties.\n",
    "np.savetxt('./Tests/test_distribution.csv', forests['Gyöngyöstarján Világos-hegy'], fmt='%s', delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE:` The cells below assume that there are 5 properties stored for each species per sampling point (nr of offered shoots, nr of freshly browsed shoots, nr of old browsed shoots, coverage in the area, supply percentage). If in the future properties are added/deleted, the cells below need to be modified accordingly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating unknown species\n",
    "\n",
    "Data on different unknown species is aggregated into a single species labeled 'Unknown'. If there are no unknown species in the forest, the columns for 'Unknown' are filled with zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forest, data in forests.items():\n",
    "    nr_species = int(np.shape(data)[1] / 5)\n",
    "    rows = len(data)\n",
    "    \n",
    "    #print(f'Original: {np.shape(data)}')\n",
    "\n",
    "    unknown_cols = []\n",
    "\n",
    "    unknown_allShoots = np.zeros((rows, 1)).astype(object)\n",
    "    unknown_allShoots[0, 0] = 'Unknown all shoots'\n",
    "\n",
    "    unknown_freshBrowsed = np.zeros((rows, 1)).astype(object)\n",
    "    unknown_freshBrowsed[0, 0] = 'Unknown freshly browsed'\n",
    "\n",
    "    unknown_oldBrowsed = np.zeros((rows, 1)).astype(object)\n",
    "    unknown_oldBrowsed[0, 0] = 'Unknown old browsed'\n",
    "\n",
    "    unknown_coverage = np.zeros((rows, 1)).astype(object)\n",
    "    unknown_coverage[0, 0] = 'Unknown forest coverage'\n",
    "\n",
    "    unknown_supplyPerc = np.zeros((rows, 1)).astype(object)\n",
    "    unknown_supplyPerc[0, 0] = 'Unknown supply percentage'\n",
    "\n",
    "    for i in range(nr_species):\n",
    "        species = data[0, i*5].lower()\n",
    "\n",
    "        if species.startswith('faj') or 'ismeretlen' in species:\n",
    "            unknown_cols.extend([i*5, i*5+1, i*5+2, i*5+3, i*5+4])\n",
    "\n",
    "    #print(f'Unknown: {unknown_cols}')\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        allShoots_columns = [col for col in unknown_cols if col % 5 == 0]\n",
    "        unknown_allShoots[i, 0] = np.sum(data[i, [allShoots_columns]])\n",
    "\n",
    "        freshBrowsed_columns = [col for col in unknown_cols if col % 5 == 1]\n",
    "        unknown_freshBrowsed[i, 0] = np.sum(data[i, [freshBrowsed_columns]])\n",
    "\n",
    "        oldBrowsed_columns = [col for col in unknown_cols if col % 5 == 2]\n",
    "        unknown_oldBrowsed[i, 0] = np.sum(data[i, [oldBrowsed_columns]])\n",
    "\n",
    "        coverage_columns = [col for col in unknown_cols if col % 5 == 3]\n",
    "        unknown_coverage[i, 0] = np.sum(data[i, [coverage_columns]])\n",
    "\n",
    "        supplyPerc_columns = [col for col in unknown_cols if col % 5 == 4]\n",
    "        unknown_supplyPerc[i, 0] = np.sum(data[i, [supplyPerc_columns]])\n",
    "\n",
    "    unknown_array = np.concatenate((unknown_allShoots, unknown_freshBrowsed, unknown_oldBrowsed, \\\n",
    "                                    unknown_coverage, unknown_supplyPerc), axis=1)\n",
    "    \n",
    "    data = np.delete(data, [unknown_cols], axis=1)\n",
    "    #print(f'Deleted: {np.shape(data)}')\n",
    "    data = np.insert(data, [0], unknown_array, axis=1)\n",
    "    #print(f'Inserted: {np.shape(data)}')\n",
    "\n",
    "    forests[forest] = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving species per forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gyöngyöstarján Világos-hegy': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza', 'Tatárjuhar'], 'Mátrabérc-Fallóskút': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza', 'Madárcseresznye', 'Mogyoró'], 'Recsk': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza', 'Mezei szil', 'Berkenye'], 'Gyöngyöspata Havas': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza', 'Vadkörte', 'Bibircses kecskerágó'], 'Gyöngyös Sár-hegy': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza', 'Vadkörte', 'Tatárjuhar', 'Molyhos tölgy'], 'Nyugat_Mátra': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza', 'Tatárjuhar', 'Mezei szil'], 'Mátra-Észak': ['Unknown', 'Kocsánytalan tölgy', 'Kocsányos tölgy', 'Csertölgy', 'Magas kőris', 'Virágos kőris', 'Gyertyán', 'Bükk', 'Hegyi juhar', 'Korai juhar', 'Mezei juhar', 'Erdei fenyő', 'Akác', 'Fagyal', 'Galagonya', 'Húsos som', 'Veresgyűrűs som', 'Kökény', 'Szeder', 'Vadrózsa', 'Bodza']}\n"
     ]
    }
   ],
   "source": [
    "species_per_forest = dict()\n",
    "\n",
    "for forest, data in forests.items():\n",
    "    forest_species = []\n",
    "\n",
    "    nr_species = int(np.shape(data)[1] / 5)\n",
    "\n",
    "    for i in range(nr_species):\n",
    "        sp = ' '.join(data[0, i*5].split(' ')[:-2])\n",
    "\n",
    "        forest_species.append(sp)\n",
    "\n",
    "    species_per_forest[forest] = forest_species\n",
    "\n",
    "print(species_per_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "species_columns = zip_longest(*species_per_forest.values(), fillvalue='')\n",
    "\n",
    "with open('species_per_forest.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    writer.writerow(species_per_forest.keys())\n",
    "\n",
    "    writer.writerows(species_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining forest data\n",
    "\n",
    "The data on different forest regions are combined into a single table consisting of sampling points as instances (rows). Columns for species which are not found in a given forest are filled with 0 values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining total number of species in all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique species in the dataset: 29\n",
      "Unique species in the dataset: ['Unknown' 'Kocsánytalan tölgy' 'Kocsányos tölgy' 'Csertölgy'\n",
      " 'Magas kőris' 'Virágos kőris' 'Gyertyán' 'Bükk' 'Hegyi juhar'\n",
      " 'Korai juhar' 'Mezei juhar' 'Erdei fenyő' 'Akác' 'Fagyal' 'Galagonya'\n",
      " 'Húsos som' 'Veresgyűrűs som' 'Kökény' 'Szeder' 'Vadrózsa' 'Bodza'\n",
      " 'Tatárjuhar' 'Madárcseresznye' 'Mogyoró' 'Mezei szil' 'Berkenye'\n",
      " 'Vadkörte' 'Bibircses kecskerágó' 'Molyhos tölgy']\n"
     ]
    }
   ],
   "source": [
    "species = np.array([])\n",
    "\n",
    "for forest, data in forests.items():\n",
    "    nr_species = int(np.shape(data)[1] / 5)\n",
    "\n",
    "    species_cols = [i * 5 for i in range(nr_species)]\n",
    "\n",
    "    for col in species_cols:\n",
    "        sp = ' '.join(data[0, col].split(' ')[:-2])\n",
    "\n",
    "        species = np.concatenate((species, np.array([sp])))\n",
    "\n",
    "unique_species, indices = np.unique(species, return_index=True)\n",
    "species = unique_species[np.argsort(indices)]\n",
    "nr_species = len(species)\n",
    "\n",
    "print(f'Number of unique species in the dataset: {nr_species}')\n",
    "print(f'Unique species in the dataset: {species}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sampling points in dataset: 2100\n"
     ]
    }
   ],
   "source": [
    "nr_rows = 0\n",
    "\n",
    "for data in forests.values():\n",
    "    nr_rows += np.shape(data)[0] - 1\n",
    "\n",
    "print(f'Total number of sampling points in dataset: {nr_rows}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing dataset with headers and all 0 values.\n",
    "dataset = np.zeros((nr_rows + 1, nr_species * 5)).astype(object)\n",
    "\n",
    "for i in range(nr_species):\n",
    "    sp = species[i]\n",
    "\n",
    "    dataset[0, i*5] = f'{sp} all shoots'\n",
    "    dataset[0, i*5 + 1] = f'{sp} freshly browsed'\n",
    "    dataset[0, i*5 + 2] = f'{sp} old browsed'\n",
    "    dataset[0, i*5 + 3] = f'{sp} forest coverage'\n",
    "    dataset[0, i*5 + 4] = f'{sp} supply percentage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_row = 1\n",
    "\n",
    "for forest, data in forests.items():\n",
    "    rows, cols = np.shape(data)\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        for j in range(cols):\n",
    "            header = data[0, j]\n",
    "\n",
    "            dataset_col = np.where(dataset[0] == header)[0][0]\n",
    "\n",
    "            dataset[dataset_row, dataset_col] = data[i, j]\n",
    "\n",
    "        dataset_row += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional processing.\n",
    "The cells below contain additional processing steps on the combined dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing freshly browsed shoots.\n",
    "To form ratios, the number of freshly browsed shoots is divided by the number of total shoots at the sampling point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = np.shape(dataset)\n",
    "\n",
    "for j in range(cols):\n",
    "    if 'freshly browsed' in dataset[0, j]:\n",
    "        for i in range(1, rows):\n",
    "            all_offered = dataset[i, j-1]\n",
    "            \n",
    "            if all_offered != 0:\n",
    "                dataset[i, j] = dataset[i, j] / all_offered\n",
    "\n",
    "            elif dataset[i,j] != 0:     # Correcting potential recording mistakes.\n",
    "                dataset[i, j] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing old browsed shoots.\n",
    "Old browsed shoots are removed from the dataset as they will not be used in the machine learning analyis. This is an optional step, if this data is needed as well, simply skip the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = dataset[0, :].tolist()\n",
    "\n",
    "old_browsed_cols = [index for index, header in enumerate(headers) if 'old browsed' in header]\n",
    "\n",
    "dataset = np.delete(dataset, [old_browsed_cols], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Hungarian to Latin names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict = {'Kocsánytalan tölgy' : 'Quercus petraea', 'Kocsányos tölgy' : 'Quercus robur', 'Csertölgy' : 'Quercus cerris',  'Magas kőris' : 'Fraxinus excelsior', \\\n",
    "    'Virágos kőris' : 'Fraxinus ornus', 'Gyertyán' : 'Carpinus betulus', 'Bükk' : 'Fagus sylvatica', 'Hegyi juhar' : 'Acer pseudoplatanus', 'Korai juhar' : 'Acer platanoides', \\\n",
    "        'Mezei juhar' : 'Acer campestre', 'Erdei fenyő' : 'Pinus sylvestris', 'Akác' : 'Robinia pseudoacacia', 'Fagyal' : 'Ligustrum vulgare ', 'Galagonya' : 'Crataegus monogyna', \\\n",
    "            'Húsos som' : 'Cornus mas', 'Veresgyűrűs som' : 'Cornus sanguinea', 'Kökény' : 'Prunus spinosa', 'Szeder' : 'Rubus fruticosus ', \\\n",
    "                'Vadrózsa' : 'Rosa canina ', 'Bodza' : 'Sambucus nigra', 'Tatárjuhar' : 'Acer tataricum', 'Madárcseresznye': 'Prunus avium ', 'Mogyoró' : 'Corylus avellana', \\\n",
    "                    'Mezei szil' : 'Ulmus minor ', 'Berkenye' : 'Sorbus aucuparia', 'Vadkörte' : 'Pyrus pyraster ', 'Bibircses kecskerágó' : 'Euonymus verrucosus', \\\n",
    "                        'Molyhos tölgy' : 'Quercus pubescens'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(np.shape(dataset)[1]):\n",
    "    species = ' '.join(dataset[0, j].split(' ')[:-2])\n",
    "    feature = ' '.join(dataset[0, j].split(' ')[-2:])\n",
    "\n",
    "    if species != 'Unknown':\n",
    "        dataset[0, j] = f'{names_dict[species]} {feature}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataset as csv.\n",
    "\n",
    "We mark it with 'raw' to allow for additional processing after data inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./Data/dataset_raw.csv', dataset, fmt='%s', delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving predictor and target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target value array: (2101, 29)\n"
     ]
    }
   ],
   "source": [
    "target_columns = [i for i in range(np.shape(dataset)[1]) if 'freshly browsed' in dataset[0, i]]\n",
    "\n",
    "target_values = dataset[:, target_columns]\n",
    "\n",
    "print(f'Shape of target value array: {np.shape(target_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./Data/freshly_browsed_raw.csv', target_values, fmt='%s', delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving target values with distinction between 0% browsed and 0 supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values_distinguished = target_values.copy()\n",
    "\n",
    "noSupply_value = -0.00001         # The value to use if there was no shoot supply.\n",
    "\n",
    "rows, cols = np.shape(target_values_distinguished)\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if dataset[i, j*4] == 0:        # If there was no supply from that species.\n",
    "            target_values_distinguished[i, j] = noSupply_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./Data/freshly_browsed_dist_raw.csv', target_values_distinguished, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictor values array: (2101, 87)\n"
     ]
    }
   ],
   "source": [
    "predictor_columns = [i for i in range(np.shape(dataset)[1]) if 'freshly browsed' not in dataset[0, i]]\n",
    "\n",
    "predictor_values = dataset[:, predictor_columns]\n",
    "\n",
    "print(f'Shape of predictor values array: {np.shape(predictor_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./Data/predictors_raw.csv', predictor_values, fmt='%s', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
